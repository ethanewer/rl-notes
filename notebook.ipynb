{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "- $s_t$ - state\n",
    "- $o_t$ - observation\n",
    "- $a_t$ - action\n",
    "- $\\pi_\\theta(a_t|o_t)$ - policy\n",
    "- $\\pi_\\theta(a_t|s_t)$ - policy (fully observed)\n",
    "- $p(s_{t+1}|s_t,a_t)$ is independant of $s_{t-1}$\n",
    "- $r(s, a)$ - reward function\n",
    "\n",
    "# Markov Chain\n",
    "$\\mathcal M = \\{\\mathcal S, \\mathcal T\\}$\n",
    "- $\\mathcal S$ - state space\n",
    "- $\\mathcal T$ - transition operator\n",
    "- $\\mathcal T_{i, j} = p(s_{t+1}= i | s_t = j)$\n",
    "- $\\mu_{t, i} = p(s_t = i)$\n",
    "- $\\mu_t$ is a vector of state probabilities\n",
    "- $\\mu_{t+1} = \\mathcal T \\mu_t$\n",
    "\n",
    "# Markov Decision Process\n",
    "$\\mathcal M = \\{\\mathcal S, \\mathcal A, \\mathcal T, r\\}$\n",
    "- $\\mathcal S$ - state space\n",
    "- $\\mathcal A$ - action space\n",
    "- $\\mathcal T$ - transition operator\n",
    "- $\\mathcal T_{i, j, k} = p(s_{t+1}= i | s_t = j, a_t = k)$\n",
    "- $r$ - reward function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "$$p(s_1, a_1, ..., s_T, a_T) = p(s_1)\\prod_{t=1}^T \\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t) = p(\\tau)$$\n",
    "\n",
    "- $\\theta^* = \\argmax_\\theta E_{p_\\theta(\\tau)}\\left[\\sum_t r(s_t, a_t)\\right]$\n",
    "\n",
    "- $p(s_{t+1}, a_{t+1}|s_t, a_t) = p(s_{t+1}|s_t, a_t)\\pi_\\theta(a_{t+1}|s_{t+1})$\n",
    "\n",
    "- $\\theta^* \n",
    "    = \\argmax_\\theta E_{p_\\theta(\\tau)}\\left[\\sum_t r(s_t, a_t)\\right]\n",
    "    = \\argmax_\\theta \\sum_t E_{p_\\theta(s_t, a_t)}[r(s_t, a_t)]$\n",
    "\n",
    "- If $T$ is unbounded and stationary distribution $p_\\theta(s, a)$ exists, \n",
    "    then $\\theta^* =  \\argmax_\\theta E_{p_\\theta(s, a)}[r(s, a)]$\n",
    "\n",
    "- Even if $r(x)$ is not smooth, we can optimize $E_{\\pi_\\theta}[r(x)]$,\n",
    "    because $E_{\\pi_\\theta}[r(x)]$ is smooth in $\\theta$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $E_{p_\\theta(\\tau)}\\left[\\sum_t r(s_t, a_t)\\right]\n",
    "    = E_{p(s_1)}[\n",
    "        E_{\\pi_\\theta(a_1|s_1)}[r(s_1, a_1) +\n",
    "            E_{p(s_2|s_1, a_1)}[\n",
    "                E_{\\pi_\\theta(a_2|s_2)}[r(s_2, a_2) +\n",
    "                    ... | s_2\n",
    "                ]\n",
    "            ] \\ | s_1\n",
    "        ]]$\n",
    "    \n",
    "- $Q(s_1, a_1) = r(s_1, a_1) + E_{p(s_2|s_1, a_1)}[\n",
    "        E_{\\pi_\\theta(a_2|s_2)}[r(s_2, a_2) +\n",
    "            ... | s_2\n",
    "        ] \\ | s_1, a_1\n",
    "    ]$\n",
    "\n",
    "- $E_{p_\\theta(\\tau)}\\left[\\sum_t r(s_t, a_t)\\right] \n",
    "    = E_{p(s_1)}[E_{\\pi_\\theta(a_1|s_1)}[Q(s_1, a_1)|s_1]]$ is easy to optimize over $a_1$ if $Q(s_1, a_1)$ is known\n",
    "\n",
    "<br>\n",
    "\n",
    "- $Q^{\\pi_\\theta}(s_t, a_t) = \\sum_{t' = t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t, a_t]$\n",
    "\n",
    "- $V^{\\pi_\\theta}(s_t) = \\sum_{t' = t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t] \n",
    "    = E_{\\pi_\\theta(a_t|s_t)}[Q^{\\pi_\\theta}(s_t, a_t)]$\n",
    "\n",
    "- $E_{p_\\theta(\\tau)}\\left[\\sum_t r(s_t, a_t)\\right] = E_{p(s_1)}[V^{\\pi_\\theta}(s_1)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "- $r(\\tau) = \\sum_t r(s_t, a_t)$\n",
    "\n",
    "- $J(\\theta) = E_{p_\\theta(\\tau)}[r(\\tau)] = \\int p_{\\theta}(\\tau)r(\\tau)d\\tau$\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta p_{\\theta}(\\tau)r(\\tau)d\\tau$\n",
    "\n",
    "- $p_\\theta(\\tau)\\nabla_\\theta \\log p_\\theta(\\tau) = p_\\theta(\\tau) \n",
    "    \\frac{\\nabla_\\theta p_\\theta(\\tau)}{p_\\theta(\\tau)} = \\nabla_\\theta p_\\theta(\\tau)$\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) \n",
    "    = \\int p_\\theta(\\tau)\\nabla_\\theta \\log p_\\theta(\\tau)r(\\tau)d\\tau\n",
    "    = E_{p_\\theta(\\tau)}[\\nabla_\\theta \\log p_\\theta(\\tau)r(\\tau)]$\n",
    "\n",
    "- $\\log p_\\theta(\\tau) = \\log \\left(p(s_1)\\prod_{t=1}^T \\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)\\right) = \\log p(s_1) + \\sum_{t=1}^T \\log \\pi_\\theta(a_t|s_t) + \\log p(s_{t+1}|s_t, a_t)$\n",
    "\n",
    "- $\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) = E_{p_\\theta(\\tau)}[r(\\tau)\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)]$\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\nabla_\\theta \\log \\pi_\\theta(\\tau_i)r(\\tau_i)$\n",
    "\n",
    "# Reducing Varience in Policy Gradient\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=1}^T \n",
    "    \\nabla_\\theta \\log \\pi_\\theta(a_{i, t}|s_{i, t})\\sum_{t'=1}^T r(s_{i, t'}, a_{i, t'})$\n",
    "\n",
    "- We can use causality to lower the variance of $\\nabla_\\theta J(\\theta)$\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=1}^T \n",
    "    \\nabla_\\theta \\log \\pi_\\theta(a_{i, t}|s_{i, t})\\sum_{t'=t}^T r(s_{i, t'}, a_{i, t'})$ from causality\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=1}^T \n",
    "    \\nabla_\\theta \\log \\pi_\\theta(a_{i, t}|s_{i, t})\\hat Q_{i, t}$\n",
    "\n",
    "- We can apply a baseline $b$ to lower the variance of $\\nabla_\\theta J(\\theta)$\n",
    "\n",
    "- $b = \\frac{1}{N}\\sum_{i=1}^N\\pi_\\theta(\\tau_i)r(\\tau_i)$\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=1}^T \n",
    "    \\nabla_\\theta \\log \\pi_\\theta(a_{i, t}|s_{i, t})(\\hat Q_{i, t} - b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling (off policy method)\n",
    "$$\\begin{align*}\n",
    "    J(\\theta) &= E_{p_\\theta(\\tau)}[r(\\tau)] \\\\\n",
    "    &= \\int p_{\\theta}(\\tau)r(\\tau)d\\tau \\\\\n",
    "    &= \\int \\frac{\\bar p(\\tau)}{\\bar p(\\tau)}p_{\\theta}(\\tau)r(\\tau)d\\tau \\\\\n",
    "    &= \\int \\bar p(\\tau)\\frac{p_{\\theta}(\\tau)}{\\bar p(\\tau)}r(\\tau)d\\tau \\\\\n",
    "    &= E_{\\bar p(\\tau)}\\left[\\frac{p_{\\theta}(\\tau)}{\\bar p(\\tau)}r(\\tau)\\right] \\\\\n",
    "    &= E_{\\bar p(\\tau)}\\left[\\frac{\n",
    "        p(s_1)\\prod_{t=1}^T \\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)\n",
    "    }{\n",
    "        p(s_1)\\prod_{t=1}^T \\bar \\pi(a_t|s_t)p(s_{t+1}|s_t, a_t)\n",
    "    }r(\\tau)\\right] \\\\\n",
    "    &= E_{\\bar p(\\tau)}\\left[\\frac{\\prod_{t=1}^T \\pi_\\theta(a_t|s_t)}{\n",
    "        \\prod_{t=1}^T \\bar \\pi(a_t|s_t)}r(\\tau)\\right] \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\nabla_{\\theta'}J(\\theta') &= \\nabla_{\\theta'}E_{p_{\\theta'}(\\tau)}[r(\\tau)] \\\\\n",
    "    &= \\nabla_{\\theta'} E_{p_\\theta(\\tau)}\\left[\\frac{p_{\\theta'}(\\tau)}{p_\\theta(\\tau)}r(\\tau)\\right] \\\\\n",
    "    &= E_{p_\\theta(\\tau)}\\left[\\frac{\\nabla_{\\theta'}p_{\\theta'}(\\tau)}{p_\\theta(\\tau)}r(\\tau)\\right] \\\\\n",
    "    &= E_{p_\\theta(\\tau)}\\left[\\frac{p_{\\theta'}(\\tau)}{p_\\theta(\\tau)}\n",
    "        \\nabla_{\\theta'}\\log p_{\\theta'}(\\tau)r(\\tau)\\right] \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic\n",
    "- $\\hat Q_{t} = \\sum_{t' = t}^T r(s_{t'}, a_{t'})$ has high variance, because it is a \n",
    "    single sample estimate of $Q^{\\pi_\\theta}(s_t, a_t)$\n",
    "\n",
    "- $Q^{\\pi_\\theta}(s_t, a_t) = \\sum_{t' = t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t, a_t]$ has much lower variance\n",
    "\n",
    "- We can also use a baseline with $Q^{\\pi_\\theta}(s_t, a_t)$\n",
    "\n",
    "- $V^{\\pi_\\theta}(s_t) = \\sum_{t' = t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t] = E_{\\pi_\\theta(a_t|s_t)}[Q^{\\pi_\\theta}(s_t, a_t)]$ \n",
    "    is a good choice of baseline\n",
    "\n",
    "- $A^{\\pi_\\theta}(s_t, a_t) = Q^{\\pi_\\theta}(s_t, a_t) - V^{\\pi_\\theta}(s_t)$ is the atvantage function \n",
    "    (gives how much better an action is compared to the expected reward at $s_t$)\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=1}^T \n",
    "    \\nabla_\\theta \\log \\pi_\\theta(a_{i, t}|s_{i, t})A^{\\pi_\\theta}(s_{i, t}, a_{i, t})$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $Q^{\\pi_\\theta}(s_t, a_t) \\approx r(s_t, a_t) + V^{\\pi_\\theta}(s_{t+1})$\n",
    "\n",
    "- $A^{\\pi_\\theta}(s_t, a_t) \\approx r(s_t, a_t) + V^{\\pi_\\theta}(s_{t+1}) - V^{\\pi_\\theta}(s_t)$\n",
    "\n",
    "- $V^{\\pi_\\theta}$ is easier to approximate because it only depends on $s_t$ rather than $s_t$ and $a_t$\n",
    "\n",
    "- We can approximate $V^{\\pi_\\theta}$ with supervised regression usinging a dataset $\\{(s_{i, t}, r(s_{i, t}, a_{i, t}) + V^{\\pi_\\phi}(s_{i, t+1}))\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from policy_grad import optimize_policy_gradient, plot_rewards\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "def noncausal_loss(logits: Tensor, a: Tensor, r: Tensor):\n",
    "    return F.cross_entropy(logits, a, reduction=\"sum\") * r.sum()\n",
    "\n",
    "\n",
    "def causal_loss(logits: Tensor, a: Tensor, r: Tensor):\n",
    "    q = torch.cumsum(r.flip(0), dim=0).flip(0)\n",
    "    return torch.sum(F.cross_entropy(logits, a, reduction=\"none\") * q)\n",
    "\n",
    "\n",
    "def causal_loss_with_baseline(logits: Tensor, a: Tensor, r: Tensor):\n",
    "    q = torch.cumsum(r.flip(0), dim=0).flip(0)\n",
    "    return torch.sum(F.cross_entropy(logits, a, reduction=\"none\") * (q - q.mean()))\n",
    "\n",
    "\n",
    "noncausal_rewards = optimize_policy_gradient(noncausal_loss, device, n_iters=750)\n",
    "causal_rewards = optimize_policy_gradient(causal_loss, device, n_iters=750)\n",
    "causal_baseline_rewards = optimize_policy_gradient(causal_loss, device, n_iters=750)\n",
    "\n",
    "plot_rewards(\n",
    "    {\n",
    "        \"Noncausal\": noncausal_rewards,\n",
    "        \"Causal\": causal_rewards,\n",
    "        \"Causal Baseline\": causal_baseline_rewards,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
